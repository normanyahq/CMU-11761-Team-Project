\subsection{Data Description}
The training set is 1000 articles, 500 real and 500 fake, of varying length. The development set is 200 articles, 100 fake, 100 real. Articles in development set is truncated to meet the length distribution in Table ~\ref{tab:040dev}. Besides these two sets, we also use a 100 million word corpus of Broadcast News articles as external source for generation of specific feature.
\begin{table}
	\begin{center}
		\begin{tabular}{|c|c|}
		\hline
		\#sentence&\#article\\
		\hline
		1&20\\
		2&10\\
		3&10\\
		4&10\\
		5&10\\
		7&10\\
		10&10\\
		15&10\\
		20&10\\
		\hline
		\end{tabular}
	\end{center}
	\label{tab:040dev}
	\caption{Article length distribution of dev set}
\end{table}
\subsection{Data Preprocessing}
Articles from training set are truncated following the document length distribution in Table ~\ref{tab:040dev}. The number of truncated training articles is 10065. Sentence per article distributions of original training set, truncated training set and development set are shown in Figure ~\ref{fig:040length}.

\begin{figure*}
\centering  
\subfigure[Training set (original)]{\includegraphics[width=0.33\linewidth]{./FIG/040/trainingdist.png}}\hfill
\subfigure[Training set (truncated)]{\includegraphics[width=0.33\linewidth]{./FIG/040/trunceddist.png}}\hfill
\subfigure[Dev set]{\includegraphics[width=0.33\linewidth]{./FIG/040/devdist.png}}\\

\caption{Article length distribution of different sets}
\label{fig:040length}
\end{figure*}

\subsection{Classifier Choice}
Popular classifiers for binary classification task are chosen as candidates, including KNN, logistic regression, SVM, gradient boosting (xgboost). Testing individual feature on development set and cross validation on training set, xgboost outperforms all the other algorithm by about 5\%. Also it is very fast comparing to SVM. So we choose xgboost as the final classier.

\subsection{Results}
The results of different feature sets are shown in Table ~\ref{tab:040res}. From the table we can tell that the statistical feature contribute most to the accuracy. Using it along could gave us a result close to the combined features. Syntactic features are not very useful in this experiment\footnote{Didn't run on dev set because of low performance on training set and large computation}. Although most semantic features fail to surpass an accuracy of 70, the combined semantic features get an accuracy of 80.
\begin{table}[!h]
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
		\hline
		Features&Avg. cv Train&Dev&Soft\\
		\hline
		Semantic & 80.2 & 80.0 & 74.9\\
		Syntactic & 52.3 & &\\
		Statistical & 85.4 &88.0&81.7\\
		All combined & 89.4 & 89.0&86.0\\
		\hline
		\end{tabular}
	\end{center}
	\label{tab:040res}
	\caption{Experiment results}
\end{table}